import { NextRequest } from 'next/server';
import { ConversationalInput, ConversationMessage, Conversation } from '@/types/conversation';
import { getTemplateById, replaceVariables } from '@/lib/templates';
import { generateWithRotation as generateWithRotationClaude } from '@/lib/anthropic';
import { generateWithRotation as generateWithRotationGemini } from '@/lib/gemini';

// Helper para enviar eventos SSE
function sendEvent(controller: ReadableStreamDefaultController, data: any) {
  const message = `data: ${JSON.stringify(data)}\n\n`;
  controller.enqueue(new TextEncoder().encode(message));
}

// Helper para gerar com provider selecionado COM RETRY
async function generateWithProvider(
  prompt: string,
  apiKeys: string[],
  provider: string,
  model?: string,
  maxRetries: number = 3
): Promise<string> {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      console.log(`üîÑ Tentativa ${attempt} de ${maxRetries}...`);

      if (provider === 'anthropic') {
        const claudeModel = model || 'claude-sonnet-4-5';
        return await generateWithRotationClaude(prompt, apiKeys, claudeModel);
      } else if (provider === 'gemini') {
        return await generateWithRotationGemini(prompt, apiKeys);
      } else {
        throw new Error(`Provider "${provider}" n√£o est√° implementado`);
      }
    } catch (error: any) {
      console.error(`‚ùå Tentativa ${attempt} falhou:`, error.message);

      // Se for a √∫ltima tentativa, lan√ßa o erro
      if (attempt === maxRetries) {
        throw error;
      }

      // Aguardar antes de tentar novamente (backoff exponencial)
      const waitTime = Math.min(5000 * Math.pow(2, attempt - 1), 30000);
      console.log(`‚è≥ Aguardando ${waitTime/1000}s antes de tentar novamente...`);
      await new Promise(resolve => setTimeout(resolve, waitTime));
    }
  }

  throw new Error('Todas as tentativas falharam');
}

// Helper para converter sele√ß√£o do usu√°rio em nome do modelo
function getClaudeModelName(userSelection: 'haiku' | 'sonnet' | 'opus'): string {
  const modelMap = {
    haiku: 'claude-haiku-4-5',
    sonnet: 'claude-sonnet-4-5',
    opus: 'claude-opus-4-1-20250805',
  };
  return modelMap[userSelection];
}

export async function POST(request: NextRequest) {
  const body = await request.json();
  const input: ConversationalInput = body;
  const resumeFrom: Conversation | undefined = body.resumeFrom;

  // Validar input
  if (!input.title || !input.synopsis) {
    return new Response(JSON.stringify({ error: 'T√≠tulo e sinopse s√£o obrigat√≥rios' }), {
      status: 400,
    });
  }

  // Pegar template
  const template = getTemplateById(input.templateId);
  if (!template) {
    return new Response(JSON.stringify({ error: 'Template n√£o encontrado' }), { status: 400 });
  }

  // Extrair API keys do provider selecionado
  const selectedProvider = input.selectedProvider;
  let providerKeys: string[] = [];

  if (['gemini', 'groq', 'cohere', 'huggingface'].includes(selectedProvider)) {
    providerKeys = input.apiKeys[selectedProvider as 'gemini' | 'groq' | 'cohere' | 'huggingface'] || [];
  } else if (['openai', 'anthropic', 'mistral', 'together', 'perplexity'].includes(selectedProvider)) {
    const singleKey = input.apiKeys[selectedProvider as 'openai' | 'anthropic' | 'mistral' | 'together' | 'perplexity'];
    if (singleKey) {
      providerKeys = [singleKey];
    }
  }

  const validKeys = providerKeys.filter((k) => k.trim().length > 0);
  if (validKeys.length === 0) {
    return new Response(JSON.stringify({ error: `Nenhuma API Key v√°lida para ${selectedProvider}` }), {
      status: 400,
    });
  }

  console.log(`üîë Usando ${validKeys.length} API key(s) do provider: ${selectedProvider}`);

  // ========== CRIAR STREAM SSE ==========
  const stream = new ReadableStream({
    async start(controller) {
      try {
        // Inicializar conversa (ou retomar)
        const conversation: Conversation = resumeFrom || {
          id: Date.now().toString(),
          templateId: input.templateId,
          createdAt: new Date(),
          messages: [],
          contextWindow: '',
          currentStepIndex: 0,
          status: 'running',
          generatedFiles: {},
          stats: {
            totalTokens: 0,
            totalChars: 0,
            estimatedCost: 0,
            duration: 0,
          },
        };

        // Atualizar status para running se estava pausado
        conversation.status = 'running';

        const startTime = Date.now();

        // Determinar em qual step come√ßar
        const startStepIndex = resumeFrom ? resumeFrom.currentStepIndex : 0;

        // Vari√°veis do template
        // OTIMIZA√á√ÉO HAIKU: Reduzir caracteres em 40% para gerar mais r√°pido
        const isHaiku = selectedProvider === 'anthropic' && input.claudeModel === 'haiku';
        const characterMultiplier = isHaiku ? 0.6 : 1.0;

        const adjustedTotalChars = Math.floor(input.totalCharacters * characterMultiplier);

        const variables = {
          TITULO: input.title,
          SINOPSE: input.synopsis,
          BASE_CONHECIMENTO: input.knowledgeBase || '',
          NUM_TOPICOS: input.numTopics,
          NUM_SUBTOPICOS: input.numSubtopics,
          IDIOMA: input.language,
          CARACTERES_TOTAIS: adjustedTotalChars,
          CARACTERES_POR_TOPICO: Math.floor(adjustedTotalChars / input.numTopics),
          CARACTERES_HOOK: Math.floor(template.variables.CARACTERES_HOOK * characterMultiplier),
        };

        if (isHaiku) {
          console.log(`‚ö° OTIMIZA√á√ÉO HAIKU: Reduzindo ${input.totalCharacters} ‚Üí ${adjustedTotalChars} caracteres (60%)`);
        }

        console.log('\nüöÄ Iniciando gera√ß√£o conversacional...');
        console.log('Template:', template.name);
        console.log('T√≥picos:', input.numTopics);
        console.log('Caracteres totais:', input.totalCharacters);

        // ========== EXECUTAR CADA STEP DO TEMPLATE ==========

        // Vari√°vel para guardar a estrutura gerada (recuperar se estiver retomando)
        let estruturaGerada = resumeFrom?.generatedFiles.estrutura || '';

        for (let stepIndex = startStepIndex; stepIndex < template.steps.length; stepIndex++) {
          const step = template.steps[stepIndex];

          // Atualizar √≠ndice atual
          conversation.currentStepIndex = stepIndex;

          // Se o step √© "topico", vamos gerar N vezes (um para cada t√≥pico)
          if (step.id === 'topico' || step.id === 'curiosidade' || step.id === 'ato') {
            // Extrair cada t√≥pico da estrutura gerada
            // REGEX MULTIL√çNGUE: Aceita "T√≥pico", "T√ìPICO", "Topic", "TOPIC", "Topico"
            const topicosRaw = estruturaGerada.split(/T[o√≥]pico? \d+:/i);

            // Remover elementos vazios (primeiro elemento ap√≥s split quando estrutura come√ßa com "T√ìPICO 1:")
            const topicos = topicosRaw.filter(t => t.trim().length > 0);

            console.log(`üîç DEBUG - Estrutura split em ${topicos.length} t√≥picos`);
            if (topicos[0]) {
              console.log(`üîç DEBUG - Primeiros 100 chars do t√≥pico 1:`, topicos[0].substring(0, 100));
            }

            // Se estiver retomando, determinar de qual t√≥pico come√ßar
            const topicosJaGerados = resumeFrom?.generatedFiles.topicos?.length || 0;
            const startTopicoNum = resumeFrom ? topicosJaGerados + 1 : 1;

            for (let topicoNum = startTopicoNum; topicoNum <= input.numTopics; topicoNum++) {
              console.log(`\nüìå Executando: ${step.name} ${topicoNum}/${input.numTopics}`);

              // Pegar a estrutura real deste t√≥pico (√≠ndice correto: topicoNum - 1)
              const topicoEstrutura = topicos[topicoNum - 1];

              if (!topicoEstrutura || topicoEstrutura.trim().length === 0) {
                console.error(`‚ùå ERRO: T√≥pico ${topicoNum} n√£o encontrado!`);
                console.error(`üìã Total de t√≥picos extra√≠dos: ${topicos.length}`);
                console.error(`üìã Estrutura completa (primeiros 500 chars):`, estruturaGerada.substring(0, 500));
                throw new Error(`T√≥pico ${topicoNum} n√£o encontrado na estrutura gerada. Verifique se a IA gerou ${input.numTopics} t√≥picos corretamente.`);
              }

              console.log(`üìù T√≥pico ${topicoNum} extra√≠do (primeiros 150 chars):`, topicoEstrutura.substring(0, 150));

              // Montar prompt para este t√≥pico espec√≠fico
              // N√ÉO adicionar "T√≥pico X:" novamente, pois j√° est√° no template original
              let promptText = replaceVariables(step.promptTemplate, {
                ...variables,
                TOPICO_NUM: topicoNum,
                TOPICO_ESTRUTURA: topicoEstrutura.trim(),
              });

              // Criar mensagem do usu√°rio
              const userMessage: ConversationMessage = {
                id: `msg-${Date.now()}-user`,
                role: 'user',
                content: promptText,
                timestamp: new Date(),
                stepId: `${step.id}${topicoNum}`,
              };

              conversation.messages.push(userMessage);

              // Enviar update
              sendEvent(controller, {
                type: 'message',
                message: userMessage,
                conversation,
                progress: ((stepIndex + topicoNum / input.numTopics) / template.steps.length) * 100,
                currentStep: `${step.name} ${topicoNum}`,
              });

              // Gerar resposta da IA
              let contextPrompt = promptText;

              if (step.usesContext && conversation.messages.length > 0) {
                // LIMITAR CONTEXTO: Pegar apenas as √∫ltimas 4 mensagens (2 pares de perguntas/respostas)
                // Isso evita que o Haiku fique sobrecarregado
                const recentMessages = conversation.messages.slice(-4);
                const contextStr = recentMessages
                  .map((m) => `${m.role === 'user' ? 'Usu√°rio' : 'Assistente'}: ${m.content}`)
                  .join('\n\n');
                contextPrompt = `${contextStr}\n\nUsu√°rio: ${promptText}`;
              }

              console.log(`üìù Tamanho do contexto: ${contextPrompt.length} caracteres`);

              const response = await generateWithProvider(
                contextPrompt,
                validKeys,
                selectedProvider,
                selectedProvider === 'anthropic' && input.claudeModel
                  ? getClaudeModelName(input.claudeModel)
                  : undefined
              );

              // Criar mensagem da IA
              const aiMessage: ConversationMessage = {
                id: `msg-${Date.now()}-ai`,
                role: 'assistant',
                content: response,
                timestamp: new Date(),
                stepId: `${step.id}${topicoNum}`,
                chars: response.length,
              };

              conversation.messages.push(aiMessage);
              conversation.stats.totalChars += response.length;

              // Salvar no arquivo correspondente
              if (!conversation.generatedFiles.topicos) {
                conversation.generatedFiles.topicos = [];
              }
              conversation.generatedFiles.topicos.push(response);

              // Enviar update
              sendEvent(controller, {
                type: 'message',
                message: aiMessage,
                conversation,
                progress: ((stepIndex + (topicoNum + 1) / input.numTopics) / template.steps.length) * 100,
              });

              // Rate limiting
              await new Promise((resolve) => setTimeout(resolve, 3000));
            }

            // Resetar flag de retomada ap√≥s processar os t√≥picos
            if (resumeFrom) {
              resumeFrom.currentStepIndex = stepIndex + 1;
            }
          } else {
            // Step normal (estrutura, hook, etc)
            console.log(`\nüìå Executando: ${step.name}`);

            let promptText = replaceVariables(step.promptTemplate, variables);

            // Criar mensagem do usu√°rio
            const userMessage: ConversationMessage = {
              id: `msg-${Date.now()}-user`,
              role: 'user',
              content: promptText,
              timestamp: new Date(),
              stepId: step.id,
            };

            conversation.messages.push(userMessage);

            sendEvent(controller, {
              type: 'message',
              message: userMessage,
              conversation,
              progress: (stepIndex / template.steps.length) * 100,
              currentStep: step.name,
            });

            // Gerar resposta
            let contextPrompt = promptText;

            if (step.usesContext && conversation.messages.length > 0) {
              // LIMITAR CONTEXTO: Pegar apenas as √∫ltimas 4 mensagens
              const recentMessages = conversation.messages.slice(-4);
              const contextStr = recentMessages
                .map((m) => `${m.role === 'user' ? 'Usu√°rio' : 'Assistente'}: ${m.content}`)
                .join('\n\n');
              contextPrompt = `${contextStr}\n\nUsu√°rio: ${promptText}`;
            }

            console.log(`üìù Tamanho do contexto: ${contextPrompt.length} caracteres`);

            const response = await generateWithProvider(
              contextPrompt,
              validKeys,
              selectedProvider,
              selectedProvider === 'anthropic' && input.claudeModel
                ? getClaudeModelName(input.claudeModel)
                : undefined
            );

            const aiMessage: ConversationMessage = {
              id: `msg-${Date.now()}-ai`,
              role: 'assistant',
              content: response,
              timestamp: new Date(),
              stepId: step.id,
              chars: response.length,
            };

            conversation.messages.push(aiMessage);
            conversation.stats.totalChars += response.length;

            // Salvar no arquivo correspondente
            if (step.outputType === 'structure') {
              conversation.generatedFiles.estrutura = response;
              estruturaGerada = response; // SALVAR ESTRUTURA PARA USAR NOS T√ìPICOS
            } else if (step.outputType === 'hook') {
              conversation.generatedFiles.hook = response;
            } else if (step.outputType === 'characters') {
              conversation.generatedFiles.personagens = response;
            } else if (step.outputType === 'soundtrack') {
              conversation.generatedFiles.trilha = response;
            } else if (step.outputType === 'takes') {
              conversation.generatedFiles.takes = response;
            }

            sendEvent(controller, {
              type: 'message',
              message: aiMessage,
              conversation,
              progress: ((stepIndex + 1) / template.steps.length) * 100,
            });

            // Rate limiting
            await new Promise((resolve) => setTimeout(resolve, 3000));
          }
        }

        // ========== CONCLU√çDO ==========
        const endTime = Date.now();
        conversation.stats.duration = Math.floor((endTime - startTime) / 1000);
        conversation.status = 'completed';

        console.log('\n‚úÖ Gera√ß√£o conversacional conclu√≠da!');
        console.log('Mensagens trocadas:', conversation.messages.length);
        console.log('Caracteres totais:', conversation.stats.totalChars);
        console.log('Dura√ß√£o:', conversation.stats.duration, 'segundos');

        sendEvent(controller, {
          type: 'complete',
          conversation,
          progress: 100,
        });

        controller.close();
      } catch (error: any) {
        console.error('‚ùå Erro na gera√ß√£o conversacional:', error);
        sendEvent(controller, {
          type: 'error',
          error: error.message,
        });
        controller.close();
      }
    },
  });

  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      Connection: 'keep-alive',
    },
  });
}
